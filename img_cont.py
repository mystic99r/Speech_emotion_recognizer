# -*- coding: utf-8 -*-
"""img_cont.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19EhoBPBaHIEIpdz4PdDd3JyoQBAaa7qE
"""

import pandas as pd   #data processing 
import numpy as np  #no of operation,vector ,numerical operation (linear algebra)
import os # to add the file 
import seaborn as sns   #visualisation of audio file
import matplotlib.pyplot as plt  # for generate pyplot
import librosa    # librosa is used for music and audio analysis 
import librosa.display   # the function constructs a plot which adaptively switches between a raw samples-based view of the signal (matplotlib.pyplot.step) and an amplitude envelope view of the signal 
## librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later

from IPython.display import Audio   # used for interactive computing # to display the audio files 
import warnings   # import warnings library 
warnings.filterwarnings('ignore')
print("Module Imported")

paths=[]
labels=[]
global i 
for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
        paths.append(os.path.join(dirname, filename))
        label=filename.split('_')[-1]   # to split the file name and get the emotion part 
        label=label.split('.')[0]   # for taking 1st emotion on 0th index
        labels.append(label.lower()) #convert emotion to lower case
        if len(paths) == 2600:  ##length of dataset
          break
        print(label)

def waveplot(data,sr,emotion):    # define waveplot function to plot the waveform the emotion 
    plt.figure(figsize=(10,4))  # to fix the size of the figure
    plt.title(emotion,size=20)   # to set the title of the emotion 
    librosa.display.waveshow(data,sr=sr)  #  To plot the amplitude envelope of a waveform  # sr is the sampling rate 
    plt.show()  #finally display the plot
def spectogram(data,sr,emotion,i):       
    x=librosa.stft(data)   # # to short-time fourier transform  (STFT)
    # stft represents a signal in the time-frequency domain by computing discrete fourier transform(DFT) over
    # short overlapping windows 
    xdb=librosa.amplitude_to_db(abs(x))    # this converts amplitude spectogram to dB-scaled spectrogram 
    plt.figure(figsize=(11,4))  # define the size of the figure 
    plt.title(emotion,size=20)   # Give the title of the emotion 
    librosa.display.specshow(xdb,sr=sr,x_axis='time',y_axis='hz')  # display spectogram  
    plt.colorbar()  # this we have done using pyplot interface 
    plt.savefig("/content/mfccsimages/" +emotion+str(i)+".png")

df=pd.DataFrame()  #data library
df['speech']=paths  #store speech in path
df['label']= labels   #store emotiom in lable
df[:5]

emotion='fear'   # assign emotion value
path=np.array(df['speech'][df['label']==emotion])[0]    ##
data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
## audio will be automatically resamples to given rate  

waveplot(data,sampling_rate,emotion)  #call the waveplot function 
spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
Audio(path)

emotion='happy'   # assign emotion value
path=np.array(df['speech'][df['label']==emotion])[0]    ##
data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
## audio will be automatically resamples to given rate  

waveplot(data,sampling_rate,emotion)  #call the waveplot function 
spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
Audio(path)

for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
      for i in range(400):
        emotion='happy'   # assign emotion value
        path=np.array(df['speech'][df['label']==emotion])[i]    ##
        data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
        ## audio will be automatically resamples to given rate  
        waveplot(data,sampling_rate,emotion)  #call the waveplot function 
        spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
        #Audio(path)

for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
      for i in range(400):
        emotion='sad'   # assign emotion value
        path=np.array(df['speech'][df['label']==emotion])[i]    ##
        data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
        ## audio will be automatically resamples to given rate  
        waveplot(data,sampling_rate,emotion)  #call the waveplot function 
        spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
        #Audio(path)

for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
      for i in range(400):
        emotion='disgust'   # assign emotion value
        path=np.array(df['speech'][df['label']==emotion])[i]    ##
        data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
        ## audio will be automatically resamples to given rate  
        waveplot(data,sampling_rate,emotion)  #call the waveplot function 
        spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
        #Audio(path)

for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
      for i in range(400):
        emotion='ps'   # assign emotion value
        path=np.array(df['speech'][df['label']==emotion])[i]    ##
        data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
        ## audio will be automatically resamples to given rate  
        waveplot(data,sampling_rate,emotion)  #call the waveplot function 
        spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
        #Audio(path)

for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
      for i in range(400):
        emotion=''   # assign emotion value
        path=np.array(df['speech'][df['label']==emotion])[i]    ##
        data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
        ## audio will be automatically resamples to given rate  
        waveplot(data,sampling_rate,emotion)  #call the waveplot function 
        spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
        #Audio(path)

for dirname, _, filenames in os.walk('/content/TESS Toronto emotional speech set data'):
    for filename in filenames:
      for i in range(400):
        emotion='neutral'   # assign emotion value
        path=np.array(df['speech'][df['label']==emotion])[i]    ##
        data,sampling_rate=librosa.load(path)   # load an audio file as a floating point time series 
        ## audio will be automatically resamples to given rate  
        waveplot(data,sampling_rate,emotion)  #call the waveplot function 
        spectogram(data,sampling_rate,emotion,i)  # calling the spectogram function 
        #Audio(path)



os.makedirs('mfccsimages')

!zip -r /content/file.zip /content/mfccsimages

from google.colab import files
files.download("/content/file.zip")

from google.colab import drive
drive.mount('/content/drive')



import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import VGG16
from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization, Dense, Conv1D
from keras.models import Model, Sequential
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.utils import np_utils

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA





import numpy as np
from tensorflow.keras import layers, models, Model, optimizers
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
import pandas as pd
from sklearn.metrics import confusion_matrix
import sys
import matplotlib.pyplot as plt
import itertools

import warnings
# ignore warnings 
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)
import keras
from keras.models import Sequential, Model, model_from_json
from keras.layers import Conv1D, MaxPooling2D, AveragePooling1D
from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization
from keras.layers import Dense, Embedding, LSTM
from keras.regularizers import l2
from keras.constraints import max_norm
from keras.callbacks import EarlyStopping, ModelCheckpoint
import cv2
from sklearn.utils import shuffle
from tensorflow.python.keras import layers, models, Model, optimizers
from tensorflow.keras import regularizers
from tensorflow.keras import layers, models, Model, optimizers
from keras.utils import np_utils, to_categorical
from random import randint
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from matplotlib import pyplot

# For training set only
import glob
angry = glob.glob('/content/drive/MyDrive/train_logmel/angry/*.*')
disgust = glob.glob('/content/drive/MyDrive/train_logmel/disgust/*.*')
fear = glob.glob('/content/drive/MyDrive/train_logmel/fear/*.*')
happy = glob.glob('/content/drive/MyDrive/train_logmel/happy/*.*')
neutral = glob.glob('/content/drive/MyDrive/train_logmel/neutral/*.*')
sad = glob.glob('/content/drive/MyDrive/train_logmel/sad/*.*')
ps = glob.glob('/content/drive/MyDrive/train_logmel/ps/*.*')
data = []
labels = []
for i in angry:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('Angry')

for i in disgust:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('Disgust')
for i in fear:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('Fear')
for i in happy:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('Happy')
for i in neutral:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('Neutral')
for i in sad:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('Sad')
for i in ps:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    data.append(image)
    labels.append('ps')
train_data = np.array(data)
train_labels = np.array(labels)
print(train_data)

X_train=train_data
y_train=train_labels

X_train.shape

angry = glob.glob('/content/drive/MyDrive/test_logmel/angry/*.*')

disgust = glob.glob('/content/drive/MyDrive/test_logmel/disgust/*.*')
fear= glob.glob('/content/drive/MyDrive/test_logmel/fear/*.*')
happy = glob.glob('/content/drive/MyDrive/test_logmel/happy/*.*')
neutral = glob.glob('/content/drive/MyDrive/test_logmel/neutral/*.*')
sad = glob.glob('/content/drive/MyDrive/test_logmel/sad/*.*')
ps = glob.glob('/content/drive/MyDrive/test_logmel/ps/*.*')

test_data = []
test_labels = []

for i in angry:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('Angry')

for i in disgust:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('Disgust')
for i in fear:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('Fear')
for i in happy:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('Happy')
for i in neutral:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('Neutral')
for i in sad:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('Sad')
for i in ps:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (224,224))
    image=np.array(image)
    test_data.append(image)
    test_labels.append('ps')

test_data = np.array(test_data)
test_labels = np.array(test_labels)
X_test=test_data

y_test=test_labels
print(X_test)

# Check shapes of training and testing sets
print('X_train has a shape of {}, y_train has a shape of {}'.format(X_train.shape,y_train.shape))
print('X_test has a shape of {}, y_test has a shape of {}'.format(X_test.shape,y_test.shape))

X_train.shape

# Finally, we normalize pixels in X_train and X_test to the range [0,1] for faster convergence
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

lb = LabelEncoder()

y_train = np_utils.to_categorical(lb.fit_transform(y_train))
y_test = np_utils.to_categorical(lb.fit_transform(y_test))

from keras.applications import VGG16

vgg_model = VGG16(weights=None,
                  include_top=False,
                  input_shape=(224, 224, 3))

for layer in vgg_model.layers:
    layer.trainable = False

x = vgg_model.output
x = Flatten()(x) # Flatten dimensions to for use in FC layers
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x) # Dropout layer to reduce overfitting
x = Dense(256, activation='relu')(x)
x = Dense(7, activation='softmax')(x) # Softmax for multiclass
transfer_model = Model(inputs=vgg_model.input, outputs=x)

# Make sure you have frozen the correct layers
for i, layer in enumerate(vgg_model.layers):
    print(i, layer.name, layer.trainable)

keras.utils.plot_model(transfer_model, show_shapes=True)

from tensorflow.keras import layers, models, Model, optimizers

learning_rate= 5e-5
transfer_model.compile(loss="categorical_crossentropy", optimizer=optimizers.Adam(learning_rate=learning_rate), metrics=["accuracy"])

# Here we use testing set as validation set
history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=2, validation_data=(X_test,y_test))

# PRINT MODEL HISTORY OF ACCURACY AND LOSS OVER EPOCHS
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
# plt.savefig('Augmented_Model_Accuracy.png')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
# plt.savefig('Augmented_Model_Loss.png')
plt.show()

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize = (5,5))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

y_pred = transfer_model.predict(X_test)
Y_pred_classes = np.argmax(y_pred,axis=1) 
Y_true = np.argmax(y_test,axis=1)
dict_characters = {0: 'angry',  1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'ps'}
confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) 
plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) 
plt.show()